
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Literal, Optional 
from graph import super_graph as app
import requests
import json
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from llm import llm

# FastAPI åº”ç”¨
api_app = FastAPI(title="Chat API", version="1.0.0")

# CORS é…ç½®
api_app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# è¯·æ±‚/å“åº”æ¨¡å‹
class QuestionRequest(BaseModel):
    question: str

class QuestionResponse(BaseModel):
    answer: str
    status: str

# æµå¼å“åº”æ¨¡å‹
class StreamResponse(BaseModel):
    content: str  # æµå¼è¾“å‡ºå†…å®¹ï¼ˆå·¥å…·æ—¥å¿—æˆ–å›ç­”ç‰‡æ®µï¼‰
    status: Literal["streaming", "tool_start", "tool_end", "info", "success", "error"]  # çŠ¶æ€æ ‡è¯†
    is_final: bool = False  # æ˜¯å¦ä¸ºæœ€ç»ˆå›ç­”ç‰‡æ®µ
    tool_name: Optional[str] = None  # å·¥å…·/å›¢é˜Ÿåç§°ï¼ˆçŠ¶æ€ä¸ºtool_start/tool_endæ—¶æœ‰æ•ˆï¼‰

@api_app.get("/api")
async def root():
    return {"message": "Chat API is running"}

@api_app.post("/api/stream")
async def chat_stream(request: QuestionRequest):
    async def generate():
        # æ„å»ºæ¶ˆæ¯
        messages = [
            SystemMessage(content="ä½ æ˜¯åŠ©æ‰‹"),  # ç³»ç»Ÿæç¤º
            HumanMessage(content=request.question)  # ç”¨æˆ·é—®é¢˜
        ]
        
        final_content = ""
        async for chunk in llm.astream(messages):
            # è·å–chunkå†…å®¹
            if hasattr(chunk, 'content') and chunk.content:
                content = chunk.content
                final_content += content
                
                # æ„å»ºå“åº”å¯¹è±¡
                stream_data = StreamResponse(
                    content=content,
                    status="streaming",
                    is_final=False  # æµå¼è¿‡ç¨‹ä¸­éƒ½ä¸æ˜¯æœ€ç»ˆå—
                )
                
                yield f"data: {json.dumps(stream_data.model_dump(), ensure_ascii=False)}\n\n"
        
        # å‘é€æœ€ç»ˆå®Œæˆä¿¡å·
        final_data = StreamResponse(
            content="",  # æœ€ç»ˆå—å¯ä»¥ç©ºå†…å®¹ï¼Œæˆ–è€…å‘é€ç»Ÿè®¡ä¿¡æ¯
            status="success", 
            is_final=True
        )
        yield f"data: {json.dumps(final_data.model_dump(), ensure_ascii=False)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
        }
    )

# å·¥å…·èŠ‚ç‚¹çŠ¶æ€æ˜ å°„å­—å…¸
TOOL_NODE_MAPPING = {
    # ç›‘ç£å†³ç­–ç±»
    "supervisor": "åˆ†æ",

    # ä¿¡æ¯æ£€ç´¢ç±»
    "search": "æœç´¢",
    "web_scraper": "ç½‘é¡µæŠ“å–",
    
    # æ–‡æ¡£å†™ä½œç±»
    "doc_writer": "æ–‡æ¡£å†™ä½œ",
    "note_taker": "ç¬”è®°æ•´ç†", 
    "chart_generator": "å›¾è¡¨ç”Ÿæˆ",
    
    # å›¢é˜ŸèŠ‚ç‚¹
    "research_team": "è°ƒç ”å›¢é˜Ÿ",
    "writing_team": "å†™ä½œå›¢é˜Ÿ",
}

def get_tool_status(event_type: str, node_name: str) -> dict:
    """
    æ ¹æ®äº‹ä»¶ç±»å‹å’ŒèŠ‚ç‚¹åç§°è·å–å·¥å…·çŠ¶æ€ä¿¡æ¯
    
    Args:
        event_type: äº‹ä»¶ç±»å‹ ('on_chain_stream', 'on_chain_end' ç­‰)
        node_name: èŠ‚ç‚¹åç§°
    
    Returns:
        dict: åŒ…å«çŠ¶æ€ä¿¡æ¯çš„å­—å…¸
    """
    # è·å–èŠ‚ç‚¹ä¸­æ–‡åç§°
    chinese_name = TOOL_NODE_MAPPING.get(node_name, 'æ€è€ƒ')
    
    # æ ¹æ®äº‹ä»¶ç±»å‹ç¡®å®šçŠ¶æ€
    if event_type == 'on_chain_stream':
        status = "streaming"
        content = f"{chinese_name}ä¸­..."
    elif event_type == 'on_chain_start':
        status = "tool_start" 
        content = f"å¼€å§‹{chinese_name}"
    elif event_type == 'on_chain_end':
        status = "tool_end"
        content = f"å®Œæˆ{chinese_name}"
    else:
        status = "info"
        content = f"{chinese_name}å¤„ç†ä¸­"
    
    return {
        "content": content,
        "status": status,
        "tool_name": node_name,
        "chinese_name": chinese_name,
        "is_final": False
    }

# åœ¨ main.py ä¸­ä½¿ç”¨ç¤ºä¾‹
@api_app.post("/api/chat")
async def chatting(request: QuestionRequest):
    async def generate_stream():
        try:
            inputs = {"messages": [HumanMessage(content=request.question)]}
            
            async for event in app.astream_events(
                inputs,
                version="v1", 
                config={"recursion_limit": 150}
            ):
                event_type = event['event']
                node_name = event.get('name', '')
                
                print(f"äº‹ä»¶ç±»å‹: {event_type}, èŠ‚ç‚¹åç§°: {node_name}")
                
                # å¤„ç†ç›‘ç£è€…çš„æµå¼è¾“å‡º
                if (event_type == 'on_chain_stream' and 
                    event.get('name') == 'supervisor'):
                    # print("event['data']", event['data'])
                    
                    chunk = event['data']['chunk']
                    
                    if hasattr(chunk, 'update') and chunk.update:
                        update_data = chunk.update
                        is_top_level = update_data.get("is_top_level", False)
                        
                        if is_top_level and "messages" in update_data and update_data["messages"]:
                            messages = update_data["messages"]
                            
                            for msg in messages:
                                if isinstance(msg, AIMessage) and msg.content:
                                    print(f"ğŸ“¤ å‘é€AIæ¶ˆæ¯: '{msg.content}'")
                                    
                                    chunk_data = StreamResponse(
                                        content=msg.content,
                                        status="streaming", 
                                        is_final=False
                                    )
                                    yield f"data: {json.dumps(chunk_data.model_dump(), ensure_ascii=False)}\n\n"
                
                # å¤„ç†å…¶ä»–èŠ‚ç‚¹çš„çŠ¶æ€é€šçŸ¥
                elif event_type in ['on_chain_start', 'on_chain_stream', 'on_chain_end']:
                    # è·å–å·¥å…·çŠ¶æ€ä¿¡æ¯
                    tool_status = get_tool_status(event_type, node_name)
                    
                    # å‘é€å·¥å…·çŠ¶æ€åˆ°å‰ç«¯
                    status_data = StreamResponse(
                        content=tool_status["content"],
                        status=tool_status["status"],
                        tool_name=tool_status["chinese_name"],
                        is_final=tool_status["is_final"]
                    )
                    yield f"data: {json.dumps(status_data.model_dump(), ensure_ascii=False)}\n\n"
            
            # æœ€ç»ˆå®Œæˆ
            final_data = StreamResponse(content="", status="success", is_final=True)
            yield f"data: {json.dumps(final_data.model_dump(), ensure_ascii=False)}\n\n"
            
        except Exception as e:
            error_data = StreamResponse(content=f"é”™è¯¯: {str(e)}", status="error", is_final=True)
            yield f"data: {json.dumps(error_data.model_dump(), ensure_ascii=False)}\n\n"
    
    return StreamingResponse(generate_stream(), media_type="text/event-stream")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(api_app, host="0.0.0.0", port=8000)