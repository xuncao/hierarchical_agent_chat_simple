from llm import llm
from typing import List, Literal, AsyncGenerator
from langchain_core.language_models.chat_models import BaseChatModel
from langgraph.graph import END
from langgraph.types import Command
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from agents import search_agent, web_scraper_agent, doc_writer_agent, note_taking_agent, chart_generating_agent
from utils import execute_agent_node
from graph.state import State

# åˆ›å»ºç›‘ç£èŠ‚ç‚¹
def make_supervisor_node(
    llm: BaseChatModel, 
    members: list[str], 
    is_top_level: bool = False  # æ ‡è®°æ˜¯å¦ä¸ºé¡¶å±‚ç›‘ç£è€…
) -> str:
    options = ["FINISH"] + members

    # ç³»ç»Ÿæç¤ºè¯å·®å¼‚åŒ–ï¼šå­ç›‘ç£è€…FINISHåè¿”å›ä¸Šçº§ï¼Œé¡¶å±‚ç›‘ç£è€…FINISHåç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
    system_prompt = (
        "ä½ æ˜¯ç›‘ç£è€…ï¼Œè´Ÿè´£ç®¡ç†ä»¥ä¸‹å·¥ä½œæ™ºèƒ½ä½“å·¥å…·æˆ–å›¢é˜Ÿä¹‹é—´çš„åä½œï¼š{members}ã€‚\n"
        "å·¥ä½œåŸåˆ™ï¼š\n"
        "1. åˆ†æå½“å‰ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿå®Œæˆå½“å‰ä»»åŠ¡ï¼š\n"
        "   - è‹¥ä¿¡æ¯æ¨¡ç³Šã€ä¸å®Œæ•´ï¼Œå¿…é¡»è°ƒç”¨æ›´åŒ¹é…çš„æ™ºèƒ½ä½“/å›¢é˜Ÿï¼›\n"
        "   - è‹¥ä¿¡æ¯å·²å……åˆ†ï¼Œè¿”å›FINISHã€‚\n"
        "2. é€‰æ‹©é€»è¾‘ï¼šä¼˜å…ˆåŒ¹é…èƒ½åŠ›æœ€åŒ¹é…çš„æˆå‘˜ï¼Œé¿å…é‡å¤è°ƒç”¨åŒä¸€æˆå‘˜ï¼Œæ— éœ€è°ƒç”¨æ‰€æœ‰æˆå‘˜ã€‚\n"
        "3. å¿…é¡»ä»å¯ç”¨åˆ—è¡¨[{members}]ä¸­é€‰æ‹©ï¼Œæˆ–è¿”å›FINISHã€‚\n"
        "4. è¾“å‡ºæ ¼å¼ï¼šä»…è¿”å›JSON {{\"next\": \"æˆå‘˜åæˆ–FINISH\"}}ï¼Œæ— å…¶ä»–å†…å®¹ã€‚\n"
        "{finish_note}"  # FINISHè¡Œä¸ºè¯´æ˜ï¼ˆæ ¹æ®å±‚çº§å·®å¼‚åŒ–ï¼‰
    ).format(
        members=", ".join(members),
        # å­ç›‘ç£è€…FINISHåè¿”å›ä¸Šçº§ç›‘ç£è€…ï¼Œé¡¶å±‚ç›‘ç£è€…FINISHåç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        finish_note="è‹¥è¿”å›FINISHï¼Œä½ éœ€è¦ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆå¹¶ç»“æŸæµç¨‹ã€‚" if is_top_level 
        else "è‹¥è¿”å›FINISHï¼Œä»£è¡¨å½“å‰å›¢é˜Ÿä»»åŠ¡å·²å®Œæˆï¼Œè¯·å°†ç»“æœè¿”å›ç»™ä¸Šçº§ç›‘ç£è€…ã€‚"
    )

    async def supervisor_node(state: State) -> Command[Literal[*members, "__end__"]]:
        history = state["messages"]
        full_response = []
        last_node = None

        if history:
            last_msg = history[-1]
            if hasattr(last_msg, "name"):
                last_node = last_msg.name
                
        messages = [SystemMessage(content=system_prompt), *history]
        
        async for chunk in llm.astream(messages):
            if chunk.content:
                full_response.append(chunk.content)

        response = ''.join(full_response).strip()
        print('ç›‘ç£è€…å“åº”:', response)

        try:
            import json
            result = json.loads(response)
            goto = result["next"]
            
            # éªŒè¯å·¥å…·åæ˜¯å¦åœ¨å¯ç”¨åˆ—è¡¨ä¸­
            if goto not in members and goto != "FINISH":
                print(f"âš ï¸ å·¥å…·å '{goto}' ä¸åœ¨å¯ç”¨åˆ—è¡¨ä¸­ï¼Œä½¿ç”¨é»˜è®¤å·¥å…·")
                goto = members[0] if members else "FINISH"
                
            # é¿å…é‡å¤è°ƒç”¨åŒä¸€èŠ‚ç‚¹
            if goto == last_node and goto in members:
                next_idx = members.index(goto) + 1
                goto = members[next_idx] if next_idx < len(members) else "FINISH"
                
        except Exception as e:
            print(f"JSONè§£æå¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å·¥å…·")
            goto = members[0] if members else "FINISH"

        if goto == "FINISH":
            print(f"ğŸ¯ ç›‘ç£è€…å†³å®š{('ç”Ÿæˆæœ€ç»ˆå›ç­”' if is_top_level else 'ç»“æŸå½“å‰å›¢é˜Ÿä»»åŠ¡')}")
            if is_top_level:
                print("ğŸ¯ è¿›å…¥é¡¶å±‚ç›‘ç£è€…çš„FINISHåˆ†æ”¯ - ä½¿ç”¨æ¶ˆæ¯æµå¼æ›´æ–°")
                
                final_answer = ""
                
                # å…ˆåˆ›å»ºä¸€ä¸ªåˆå§‹æ¶ˆæ¯
                initial_update = {
                    "messages": [*history, AIMessage(content="")],
                    "final_answer": ""
                }
                yield Command(goto=None, update=initial_update)
                
                print("ğŸ”´ å¼€å§‹çœŸæ­£çš„æµå¼ç”Ÿæˆ...")
                
                # æµå¼æ›´æ–°æ¶ˆæ¯å†…å®¹
                async for chunk in generate_final_answer_stream(llm, history):
                    print(f"ğŸŸ¢ å®æ—¶chunk: '{chunk}'")
                    final_answer += chunk
                    
                    # å®æ—¶æ›´æ–°æœ€åä¸€ä¸ªæ¶ˆæ¯çš„å†…å®¹
                    yield Command(
                        goto=None,
                        update={
                            "messages": [*history, AIMessage(content=chunk)],
                            "final_answer": final_answer,
                            "is_top_level": True,
                        }
                    )
                
                # å®Œæˆåè·³è½¬
                yield Command(
                    goto=END,
                    update={
                        "messages": [*history, AIMessage(content='')],
                        "final_answer": final_answer,
                        "is_top_level": True
                    }
                )
            else:
                final_answer = await generate_final_answer(llm, history)
                yield Command(
                    goto=END,
                    update={
                        "messages": [*history, AIMessage(content=final_answer)],
                        "final_answer": final_answer,
                        "is_top_level": False
                    }
                )
            return
        else:
            print(f"âœ… ç›‘ç£è€…å†³å®šä¸‹ä¸€æ­¥: {goto}")
            yield Command(goto=goto, update={"next": goto})

    return supervisor_node

def _build_answer_prompt(history: list) -> str:
    """æ„å»ºå›ç­”æç¤ºè¯"""
    user_question = ""
    for msg in history:
        if isinstance(msg, HumanMessage) and not hasattr(msg, 'name'):
            user_question = msg.content
            break
    
    return f"""
        åŸºäºä»¥ä¸‹å¯¹è¯å†å²ï¼Œè¯·ç›´æ¥ã€å®Œæ•´åœ°å›ç­”ç”¨æˆ·çš„åŸå§‹é—®é¢˜ã€‚
        ç”¨æˆ·åŸå§‹é—®é¢˜ï¼š{user_question}
        å¯¹è¯å†å²ï¼š
        {chr(10).join([f"- {type(msg).__name__}: {msg.content}" for msg in history if hasattr(msg, 'content')])}
        è¯·æä¾›å®Œæ•´ã€å‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆï¼š
    """

async def generate_final_answer(llm: BaseChatModel, history: list) -> str:
    """ç”Ÿæˆæœ€ç»ˆå›ç­”å¹¶è¿”å›å†…å®¹"""
    answer_prompt = _build_answer_prompt(history)
    messages = [HumanMessage(content=answer_prompt)]

    response = ""
    
    print("ğŸ¤– ç”Ÿæˆæœ€ç»ˆå›ç­”...")
    async for chunk in llm.astream(messages):
        if chunk.content:
            response += chunk.content
            print(chunk.content, end="", flush=True)
    
    print("\nâœ… æœ€ç»ˆå›ç­”ç”Ÿæˆå®Œæˆ")
    return response.strip()
    
async def generate_final_answer_stream(
    llm: BaseChatModel,
    history: list
) -> AsyncGenerator[str, None]:
    answer_prompt = _build_answer_prompt(history)
    messages = [HumanMessage(content=answer_prompt)]

    print("ğŸ¤– ç”Ÿæˆæœ€ç»ˆå›ç­”...")
    async for chunk in llm.astream(messages):
        if chunk.content:
            yield chunk.content

# async def search_node(state: State) -> Command[Literal["supervisor"]]:
#     """æœç´¢èŠ‚ç‚¹"""
#     async for cmd in execute_agent_node(search_agent, state, "search", "ğŸ” è”ç½‘æœç´¢"):
#         yield cmd

async def search_node(state: State) -> Command[Literal["supervisor"]]:
    """æœç´¢èŠ‚ç‚¹"""
    return await execute_agent_node(search_agent, state, "search", "ğŸ” æœç´¢èŠ‚ç‚¹")

async def web_scraper_node(state: State) -> Command[Literal["supervisor"]]:
    """ç½‘é¡µæŠ“å–èŠ‚ç‚¹"""
    return await execute_agent_node(web_scraper_agent, state, "web_scraper", "ğŸŒ ç½‘é¡µæŠ“å–èŠ‚ç‚¹")

# è°ƒç ”ç›‘ç£èŠ‚ç‚¹
research_supervisor_node = make_supervisor_node(llm, ["search", "web_scraper"], is_top_level=False)

async def doc_writing_node(state: State) -> Command[Literal["supervisor"]]:
    """å†™æ–‡æ¡£èŠ‚ç‚¹"""
    return await execute_agent_node(doc_writer_agent, state, "doc_writer", "ğŸ“ å†™æ–‡æ¡£èŠ‚ç‚¹")

async def note_taking_node(state: State) -> Command[Literal["supervisor"]]:
    """å†™å¤§çº²èŠ‚ç‚¹"""
    return await execute_agent_node(note_taking_agent, state, "note_taker", "ğŸ“„ å†™å¤§çº²èŠ‚ç‚¹")

async def chart_generating_node(state: State) -> Command[Literal["supervisor"]]:
    """å†™å›¾è¡¨ä»£ç èŠ‚ç‚¹"""
    return await execute_agent_node(chart_generating_agent, state, "chart_generator", "ğŸ“ˆ å†™å›¾è¡¨ä»£ç èŠ‚ç‚¹")

# å†™ä½œç›‘ç£èŠ‚ç‚¹
doc_writing_supervisor_node = make_supervisor_node(llm, ["doc_writer", "note_taker", "chart_generator"], is_top_level=False)

# åˆ›å»ºé¡¶å±‚ç›‘ç£è€…èŠ‚ç‚¹ï¼šç®¡ç† research_team å’Œ writing_team ä¸¤ä¸ªå­å›¢é˜Ÿ
teams_supervisor_node = make_supervisor_node(llm, ["research_team", "writing_team"], is_top_level=True)
